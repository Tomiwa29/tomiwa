{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomiwa29/tomiwa/blob/master/Suspicious%20transaction_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBkfYcvprKLH"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Scenario:\n",
        "\n",
        "\n",
        "Goal:\n",
        "Detect suspicious transactions using Ml\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcqh6trxsguG"
      },
      "source": [
        "# **Importing necessary liberies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bh84kklfkjf"
      },
      "outputs": [],
      "source": [
        "! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yujp6n97piGi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from pandas_profiling import ProfileReport\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilywU0n-tY5q"
      },
      "outputs": [],
      "source": [
        "# Reading the Dataset\n",
        "\n",
        "df = pd.read_csv(\"/content/PS_20174392719_1491204439457_log.csv\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiXcozhbuOCY"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdyQkPJSvu4x"
      },
      "outputs": [],
      "source": [
        "df.describe()\n",
        "# is a Pandas method that computes a summary of statistics for each column in a DataFrame. It returns a new DataFrame containing the following statistics:\n",
        "#count: the number of non-null values in each column mean: the mean of each column std: the standard deviation of each column min: the minimum value in each column 50%: the 50th percentile (median) of each column 75%: the 75th percentile of each column max: the maximum value in each column\n",
        "#This method can be useful for gaining a quick understanding of the distribution of values in each column of a DataFrame, and for detecting potential issues such as missing or outlier values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXT1N9yAV0UN"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW_xETDSdACS"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3rnfSi3duC6"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvJnIgjLhFdS"
      },
      "outputs": [],
      "source": [
        "df.isnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7QvF7GUhTBN"
      },
      "outputs": [],
      "source": [
        "list(df.loc[df.isFraud == 0].type.drop_duplicates().values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5u1oUs7lQ7T"
      },
      "outputs": [],
      "source": [
        "list(df.loc[df.isFraud == 1].type.drop_duplicates().values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pobdJEben76m"
      },
      "outputs": [],
      "source": [
        "dft = df.loc[(df.isFraud == 1) & (df.type == 'TRANSFER')]\n",
        "dfc = df.loc[(df.isFraud == 1) & (df.type == 'CASH_OUT')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw4khzs7oFOc"
      },
      "outputs": [],
      "source": [
        "#Number of fraudulent TRANSFER\n",
        "len(dft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8ioN2F7pjYP"
      },
      "outputs": [],
      "source": [
        "#Number of fraudulent CASH_OUT\n",
        "len(dfc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnNqsGHQpYeQ"
      },
      "outputs": [],
      "source": [
        "def prep_data(df: pd.DataFrame) -> (np.ndarray, np.ndarray):\n",
        "    \"\"\"\n",
        "    Convert the DataFrame into two variable\n",
        "    X: data columns (V1 - V28)\n",
        "    y: lable column\n",
        "    \"\"\"\n",
        "    X = df.iloc[:, 2:30].values\n",
        "    y = df.Class.values\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyfInSBvwMyU"
      },
      "outputs": [],
      "source": [
        "df.profile_report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUzE2o2HKayy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le=LabelEncoder()\n",
        "\n",
        "df['nameOrig']=le.fit_transform(df['nameOrig'])\n",
        "df['type']=le.fit_transform(df['type'])\n",
        "df['nameDest']=le.fit_transform(df['nameDest'])\n",
        "\n",
        "df.head(9)\n",
        "# 3 is payment 4 is TRANSFER 1 is CASHOUT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uUoKnKBzVGq"
      },
      "source": [
        "We check if their is a good proportion between our positive and negative binary predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZk4rHKwwZ0c"
      },
      "outputs": [],
      "source": [
        "df['isFraud'].value_counts()\n",
        "#This means that there are 137993 rows where the value in the isFraud column is 0 and 122 rows where the value is 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idq7bYWrzkeQ"
      },
      "source": [
        "# Create a histogram of transaction amounts\n",
        "plt.hist(df['transaction_amount'], bins=50)\n",
        "plt.xlabel('Transaction Amount')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-0c5pg7cPov"
      },
      "outputs": [],
      "source": [
        "# Create a histogram of transaction amounts\n",
        "plt.hist(df['amount'], bins=20)\n",
        "plt.xlabel('amount')\n",
        "plt.ylabel('frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RImAQiNUdWoe"
      },
      "outputs": [],
      "source": [
        "# Create a scatterplot of transaction amounts by time\n",
        "sns.scatterplot(x='type', y='amount', hue='isFraud', data=df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoI0uCmP0bws"
      },
      "source": [
        "### Correlation Matrix\n",
        "Correlation Matrix lets you see correlations between all variable. How highly or weak each variable is related to each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDw9iLe4uuDS"
      },
      "outputs": [],
      "source": [
        "fraud_people = df[df['isFraud']==1]\n",
        "legit_people = df[df['isFraud']==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh54jb8qux-K"
      },
      "outputs": [],
      "source": [
        "fraud_people.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNI4r_giu1g6"
      },
      "outputs": [],
      "source": [
        "legit_people.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyM42M4tu5m1"
      },
      "outputs": [],
      "source": [
        "fraud_people['amount'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBqbRoP2Join"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHRPY7rnvD5M"
      },
      "outputs": [],
      "source": [
        "graph, (plot1, plot2) = plt.subplots(2,1,sharex = True)\n",
        "graph.suptitle(\"Average amount per class\")\n",
        "bins = 70\n",
        "\n",
        "plot1.hist(fraud_people['amount'], bins = bins)\n",
        "plot1.set_title('Fraud Amount')\n",
        "\n",
        "plot2.hist(legit_people['amount'], bins = bins)\n",
        "plot2.set_title('Legit Amount')\n",
        "\n",
        "plt.xlabel('amount')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.yscale('log')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlQT2yFLlLFv"
      },
      "outputs": [],
      "source": [
        "columns = df.columns.tolist()\n",
        "columns = [var for var in columns if var not in [\"isFraud\"]] #independent\n",
        "target = \"isFraud\"\n",
        "x = df[columns]\n",
        "y = df[target] #dependent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPNA1ETPvi68"
      },
      "outputs": [],
      "source": [
        "x['type'].replace(['CASH_IN','CASH_OUT','DEBIT','PAYMENT','TRANSFER'],['1','2','3','4','5',], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciRbOipmwAgv"
      },
      "outputs": [],
      "source": [
        "x.drop(['nameOrig','nameDest','isFlaggedFraud'],axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbPJ1KA5wX1T"
      },
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7lwX-0swd9f"
      },
      "outputs": [],
      "source": [
        "x.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpv1DWbTAYmW"
      },
      "source": [
        "By comparing positve and negative patients, we can see there are vast differences in means for many of our 13 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATC9e554_UWM"
      },
      "outputs": [],
      "source": [
        "# Add random noise to the features\n",
        "noise = np.random.normal(loc=0, scale=0.1, size=x.shape)  # Adjust the noise scale as needed\n",
        "x_noisy = x + noise\n",
        "\n",
        "# Introduce outliers\n",
        "outliers = np.random.uniform(low=-10, high=10, size=(10, x.shape[1]))  # Generate 10 outlier instances\n",
        "x_outliers = np.concatenate((x, outliers), axis=0)\n",
        "y_outliers = np.concatenate((y, np.ones((10,))), axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSrZCaNTF5Uw"
      },
      "source": [
        "## Prepare Data for Modeling\n",
        "To prepare data for modeling, remember ASN (Assign, Split, Normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfS5gO-hMBea"
      },
      "outputs": [],
      "source": [
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QX3YMAZLknr"
      },
      "outputs": [],
      "source": [
        "df.dropna(subset=['isFraud'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYqV-iR5NBfs"
      },
      "outputs": [],
      "source": [
        "missing_values = y.isnull().sum()\n",
        "print(\"Missing values in y:\", missing_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5Ho8kQ5N4It"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Impute missing target value using mean imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF_Mp0kZOMY3"
      },
      "outputs": [],
      "source": [
        "print(y.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImOya3gJOSnb"
      },
      "outputs": [],
      "source": [
        "print(y[y.isnull()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCtAoCFrOgR7"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Initialize SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Impute missing values in y\n",
        "y_imputed = imputer.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Convert back to a pandas Series\n",
        "y_imputed = pd.Series(y_imputed.flatten(), index=y.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOaVY0UdO3nb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "# Initialize Binarizer\n",
        "binarizer = Binarizer(threshold=0.5)  # Adjust the threshold as needed\n",
        "\n",
        "# Binarize the target variable\n",
        "y_binarized = binarizer.fit_transform(y_imputed.values.reshape(-1, 1))\n",
        "\n",
        "# Convert back to a pandas Series\n",
        "y_binarized = pd.Series(y_binarized.flatten(), index=y.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPLepGY6O7nd"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Initialize RandomOverSampler\n",
        "resampler = RandomOverSampler()\n",
        "\n",
        "# Resample the data\n",
        "x_resampled, y_resampled = resampler.fit_resample(x, y_binarized)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize RandomOverSampler\n",
        "resampler = RandomOverSampler()\n",
        "\n",
        "# Resample the data\n",
        "x_resampled, y_resampled = resampler.fit_resample(x, y_binarized)\n",
        "\n",
        "# Initialize SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Impute missing values in x_resampled\n",
        "x_resampled_imputed = imputer.fit_transform(x_resampled)\n",
        "\n",
        "# Split the resampled and imputed data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_resampled_imputed, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier on the training set\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap for visualization\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "\n",
        "# Add labels, title, and axis ticks\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# Save the figure as an image\n",
        "plt.savefig('confusion_matrix.png')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Calculate R^2 score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R^2 Score:\", r2)\n",
        "\n"
      ],
      "metadata": {
        "id": "eE1KRlS9yOgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate predicted probabilities for the positive class\n",
        "y_pred_proba_rf = model.predict(x_test)\n",
        "y_pred_proba_rf = y_pred_proba_rf.flatten()  # Flatten the predictions to a 1D array\n",
        "\n",
        "# Calculate false positive rate, true positive rate, and thresholds\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_proba_rf)\n",
        "\n",
        "# Calculate the AUC score\n",
        "auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.plot(fpr_rf, tpr_rf, label='Random Forest (AUC = {:.2f})'.format(auc_rf))\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic - Random Forest')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e6OHA5wgtSBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize and fit K-means clustering algorithm\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(x_resampled_imputed)\n",
        "\n",
        "# Predict cluster labels\n",
        "cluster_labels = kmeans.predict(x_resampled_imputed)\n",
        "\n",
        "# If ground truth labels are not available, calculate accuracy using an alternative method\n",
        "if 'target_variable' not in y_resampled:\n",
        "    # Calculate the majority label in each cluster\n",
        "    cluster_majority_label = []\n",
        "    for cluster in range(2):\n",
        "        cluster_samples = y_resampled[cluster_labels == cluster]\n",
        "        cluster_majority_label.append(cluster_samples.mode()[0])\n",
        "\n",
        "    # Assign the predicted label based on the majority label in each cluster\n",
        "    y_pred = [cluster_majority_label[label] for label in cluster_labels]\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_resampled, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "    from sklearn.cluster import KMeans\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_resampled, y_pred)\n",
        "\n",
        "# Create a heatmap for visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_resampled, y_pred)\n",
        "recall = recall_score(y_resampled, y_pred)\n",
        "f1 = f1_score(y_resampled, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error,r2_score\n",
        "\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_resampled, y_pred)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_resampled, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Calculate R^2 score\n",
        "r2 = r2_score(y_resampled, y_pred)\n",
        "print(\"R^2 Score:\", r2)"
      ],
      "metadata": {
        "id": "FooWoIRUpCBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Handle missing values in x_resampled\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "x_resampled_imputed = imputer.fit_transform(x_resampled)\n",
        "\n",
        "# Perform feature scaling on x_resampled_imputed\n",
        "scaler = StandardScaler()\n",
        "x_resampled_scaled = scaler.fit_transform(x_resampled_imputed)\n",
        "\n",
        "# Fit K-means on the scaled data\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(x_resampled_scaled)\n",
        "\n",
        "# Calculate the distances to the cluster centroids\n",
        "distances = kmeans.transform(x_resampled_scaled)\n",
        "\n",
        "# Use the minimum distance as the anomaly score\n",
        "anomaly_scores = np.min(distances, axis=1)\n",
        "\n",
        "# Calculate false positive rate, true positive rate, and thresholds\n",
        "fpr_kmeans, tpr_kmeans, thresholds_kmeans = roc_curve(y_resampled, anomaly_scores)\n",
        "\n",
        "# Calculate the AUC score\n",
        "auc_kmeans = roc_auc_score(y_resampled, anomaly_scores)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.plot(fpr_kmeans, tpr_kmeans, label='K-means (AUC = {:.2f})'.format(auc_kmeans))\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic - K-means')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7_7kNd_Gt8aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRCjjCQ-QMHH"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Initialize and train the K-means clustering algorithm\n",
        "kmeans = KMeans(n_clusters=3)  # Adjust the number of clusters as needed\n",
        "kmeans.fit(x_resampled_imputed)\n",
        "\n",
        "# Obtain the cluster assignments for the data points\n",
        "cluster_labels = kmeans.predict(x_resampled_imputed)\n",
        "\n",
        "# Perform further analysis or processing on the cluster assignments or centroids\n",
        "# ..\n",
        "\n",
        "# Evaluate the performance of the clustering algorithm\n",
        "silhouette_score_value = silhouette_score(x_resampled_imputed, cluster_labels)\n",
        "print(\"Silhouette Score:\", silhouette_score_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Handle missing values in x_resampled\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "x_resampled_imputed = imputer.fit_transform(x_resampled)\n",
        "\n",
        "# Perform feature scaling on x_resampled_imputed\n",
        "scaler = StandardScaler()\n",
        "x_resampled_scaled = scaler.fit_transform(x_resampled_imputed)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_resampled_scaled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the ANN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred_ann_probs = model.predict(x_test)\n",
        "y_pred_ann = (y_pred_ann_probs > 0.5).astype(int)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred_ann)\n",
        "recall = recall_score(y_test, y_pred_ann)\n",
        "f1 = f1_score(y_test, y_pred_ann)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, y_pred_ann)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred_ann)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Calculate R^2 score\n",
        "r2 = r2_score(y_test, y_pred_ann)\n",
        "print(\"R^2 Score:\", r2)\n"
      ],
      "metadata": {
        "id": "cQf-vFH8qT_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate predicted probabilities for the positive class\n",
        "y_pred_proba_ann = model_ann.predict(x_test)\n",
        "\n",
        "# Calculate false positive rate, true positive rate, and thresholds\n",
        "fpr_ann, tpr_ann, thresholds_ann = roc_curve(y_test, y_pred_proba_ann)\n",
        "\n",
        "# Calculate the AUC score\n",
        "auc_ann = roc_auc_score(y_test, y_pred_proba_ann)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.plot(fpr_ann, tpr_ann, label='ANN (AUC = {:.2f})'.format(auc_ann))\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic - ANN')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VF2kLhUHs-np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize and fit the MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10, 10), random_state=42)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = mlp.predict(x_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create the confusion matrix plot using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "\n",
        "# Set plot labels\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_zqQeroO2QDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize and fit the MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10, 10), random_state=42)\n",
        "mlp.fit(x_train, y_train)\n",
        "\n",
        "# Generate predicted probabilities for the positive class\n",
        "y_pred_proba = mlp.predict_proba(x_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot precision-recall curve\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5AjZyIsN3zx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define individual classifiers\n",
        "random_forest = RandomForestClassifier(random_state=42)\n",
        "ann = MLPClassifier(random_state=42)\n",
        "\n",
        "# Define the ensemble classifier\n",
        "ensemble_classifier = VotingClassifier(\n",
        "    estimators=[('rf', random_forest), ('ann', ann)],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "# Fit the ensemble classifier on the training data\n",
        "ensemble_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions using the ensemble classifier\n",
        "y_pred = ensemble_classifier.predict(x_test)\n",
        "\n",
        "# Evaluate the performance of the ensemble classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R^2 Score:\", r2)\n",
        "\n",
        "# Calculate and plot confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a confusion matrix plot\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "\n",
        "tick_marks = np.arange(2)\n",
        "plt.xticks(tick_marks, ['Class 0', 'Class 1'])\n",
        "plt.yticks(tick_marks, ['Class 0', 'Class 1'])\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "\n",
        "# Add labels to each cell\n",
        "thresh = cm.max() / 2.0\n",
        "for i, j in np.ndindex(cm.shape):\n",
        "    plt.text(j, i, format(cm[i, j], 'd'),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P2jHEEqPK6D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Generate a synthetic classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit KMeans clustering on the training set\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "# Obtain cluster labels for the training and testing sets\n",
        "X_train_clusters = kmeans.transform(X_train)\n",
        "X_test_clusters = kmeans.transform(X_test)\n",
        "\n",
        "# Concatenate the original features with the cluster distances\n",
        "X_train_new = np.concatenate((X_train, X_train_clusters), axis=1)\n",
        "X_test_new = np.concatenate((X_test, X_test_clusters), axis=1)\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fit the Random Forest classifier on the training set with new features\n",
        "rf_classifier.fit(X_train_new, y_train)\n",
        "\n",
        "# Make predictions on the testing set with new features\n",
        "y_pred_rf = rf_classifier.predict(X_test_new)\n",
        "\n",
        "# Calculate performance metrics for Random Forest\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "precision_rf = precision_score(y_test, y_pred_rf)\n",
        "recall_rf = recall_score(y_test, y_pred_rf)\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate confusion matrix for Random Forest\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Random Forest Metrics:\")\n",
        "print(\"Accuracy:\", accuracy_rf)\n",
        "print(\"Precision:\", precision_rf)\n",
        "print(\"Recall:\", recall_rf)\n",
        "print(\"F1-score:\", f1_rf)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_rf)\n"
      ],
      "metadata": {
        "id": "6tKNeywgOjuA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}